{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import emoji"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def stl(word):\n",
    "\tnlist = np.zeros((4, 27))\n",
    "\talphabet = list(string.ascii_lowercase)\n",
    "\tif len(word) < 4:\n",
    "\t\tword += ' '*(4-len(word))\n",
    "\tfor i in range(4):\n",
    "\t\tif word[i] == ' ':\n",
    "\t\t\tnlist[i][26] = 1.0\n",
    "\t\telse:\n",
    "\t\t\talphabet_index = alphabet.index(word[i])\n",
    "\t\t\tnlist[i][alphabet_index] = 1.0\n",
    "\treturn nlist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "emojis = np.identity(7) # 0: hat, 1: rat, 2: cat, 3: flat, 4: matt, 5: cap, 6: son\n",
    "\n",
    "x_train = torch.tensor([stl('hat '), stl('rat '), stl('cat '), stl('flat'), stl('matt'), stl('cap '), stl('son ')]).float()\n",
    "\n",
    "y_train = torch.tensor([emojis[0], emojis[1], emojis[2], emojis[3], emojis[4], emojis[5], emojis[6]]).float()\n",
    "\n",
    "index_to_emoji = ['hat', 'rat', 'cat', 'flat', 'matt', 'cap', 'son']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "class LongShortTermMemoryModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LongShortTermMemoryModel, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(108, 128, batch_first=True)  # 128 is the state size\n",
    "        self.dense = nn.Linear(128, 7)  # 128 is the state size\n",
    "\n",
    "    def reset(self):  # Reset states prior to new input sequence\n",
    "        zero_state = torch.zeros(1, 1, 128).float()  # Shape: (number of layers, batch size, state size)\n",
    "        self.hidden_state = zero_state\n",
    "        self.cell_state = zero_state\n",
    "\n",
    "    def logits(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        out, (self.hidden_state, self.cell_state) = self.lstm(x, (self.hidden_state, self.cell_state))\n",
    "        # print(out.size())\n",
    "        return self.dense(out.reshape(-1, 128))\n",
    "\n",
    "    def f(self, x):  # x shape: (sequence length, batch size, encoding size)\n",
    "        return torch.softmax(self.logits(x), dim=1)\n",
    "\n",
    "    def loss(self, x, y):  # x shape: (sequence length, batch size, encoding size), y shape: (sequence length, encoding size)\n",
    "        # print(y)\n",
    "        # print(self.logits(x).size())\n",
    "        # print(y.argmax(1))\n",
    "        return nn.functional.cross_entropy(self.logits(x), y.argmax(1))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "model = LongShortTermMemoryModel()\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "for epoch in range(500):\n",
    "    for batch in range(7):\n",
    "        model.reset()\n",
    "        model.loss(x_train[batch].reshape(1,1,-1), y_train[batch].reshape(1,-1)).backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "for index in range(7):\n",
    "\ty = torch.tensor([stl(index_to_emoji[index])]).reshape(1,1,-1).float()\n",
    "\tprint(emoji.emojize(f'Input: {index_to_emoji[index]}, Predicted: :{index_to_emoji[model.f(y).argmax(1)]}:'))\n",
    "index = torch.tensor([stl('rt')]).reshape(1,1,-1).float()\n",
    "print(emoji.emojize(f'Input: rt, Predicted: :{index_to_emoji[model.f(index).argmax(1)]}:'))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: hat, Predicted: :hat:\n",
      "Input: rat, Predicted: 🐀\n",
      "Input: cat, Predicted: 🐈\n",
      "Input: flat, Predicted: :flat:\n",
      "Input: matt, Predicted: :matt:\n",
      "Input: cap, Predicted: :cap:\n",
      "Input: son, Predicted: :son:\n",
      "Input: rt, Predicted: 🐀\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "rat = \"rat\"\n",
    "print(\"\\N{hat}\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 0-6: unknown Unicode character name (3406385143.py, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_52012/3406385143.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(\"\\N{hat}\")\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 0-6: unknown Unicode character name\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}